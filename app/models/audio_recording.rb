class AudioRecording < Artefact
#  include Neo4j::ActiveNode
#  property :name, type: String
  property :md5AtSave, type: String
  property :waveformArray, type: String
  property :durationInSeconds, type: Float

  ## Sample
  # file = File.read('./some-file.json') #generated by `./audiowaveformLinux -i Stephanie\ -\ 2016-03-21\ -\ Office,\ Speakers\ \(Sticker\ shock\),\ meeting\ Kate\ in\ Person.mp3 --pixels-per-second 1 -b 8 -o some-file.json`
  # data_hash = JSON.parse(file)
  # rec = AudioRecording.first
  # rec.waveformArrayJSON = data_hash['data'].every_nth(5).collect { |n| n.to_f / 100 }
    # to_f forces it to be a float
    # / 100 scales it down. Wavesurfer expects -1..1
    # every_nth(5) is a monkey-patched function that returns every 5th element of the array, resulting in about 800 samples from 3600  

  has_neo4jrb_attached_file :ingestedFile
  validates_attachment_content_type :ingestedFile, content_type: /\Aaudio\/.*\Z/
  has_neo4jrb_attached_file :ingestedVisualizationFile
  validates_attachment_content_type :ingestedVisualizationFile, content_type: /\Aimage\/.*\Z/

  has_neo4jrb_attached_file :ingestedFileWaveformArrayJSON 
#  validates_attachment_content_type :ingestedFileWaveformArrayJSON, content_type: ["/\Atext\/.*\Z/", "application/json"]
  do_not_validate_attachment_file_type :ingestedFileWaveformArrayJSON

  ## Ideas for JSON generation:
  # 1. change all keys and hashes to lowercase
  # 2. Include more data, such as OS version

  has_many :in, :origionalArtefactFragments, type: :ORIGINAL_AUDIORECORDING, model_class: :AudioRecordingFragment

  ## Note: TIMEZONES
  # Ruby will automatically find the GMT offset if you use this format:
  # Time.zone = 'America/Halifax'
  # Time.zone.parse(String)
  # Note: Time.zone.parse("121028 215320") will return: 
    # "Sun, 28 Oct 0012 21:53:20 LMT -04:14" (0012, not 2012) aka -61762427536
    # When possible, use 4-digit dates

  def self.returnTrue
    return true
  end

  def returnTrue2
    return true
  end


  def getWaveSurferArray
    # Has to be called after the audio file has been saved through ingest
    # Find the duration of the file
      #self.durationInSeconds
    # arrive at 800 samples:
      if self.durationInSeconds >= 800
        pixelsPerSecond = 1
        skipAmount = (self.durationInSeconds / 800).ceil #ceil always rounds up
      elsif self.durationInSeconds.between?(100, 800)
        # Calculate pixels per second as 800 / duration
        pixelsPerSecond = (800 / self.durationInSeconds).ceil
        # No skip needed
        # skipAmount = nil #Already nil; not defined
      else 
        pixelsPerSecond = 4
      end
    puts "Audiowaveform start"
    puts `audiowaveform -i "#{File.absolute_path(self.ingestedFile.path)}" --pixels-per-second #{pixelsPerSecond} -b 8 -o "/tmp/#{File.basename(self.ingestedFile.path)}.json"`
    puts "Audiowaveform end"
    
    # save JSON to ingestedFileWaveformArrayJSON
    jsonTempFile = File.open("/tmp/#{File.basename(self.ingestedFile.path)}.json")
    puts "File.exist?(jsonTempFile): #{File.exist?(jsonTempFile)}"
    self.ingestedFileWaveformArrayJSON = jsonTempFile
    self.save
    
    # Parse tempfile.json
    data_hash = JSON.parse(Paperclip.io_adapters.for(self.ingestedFileWaveformArrayJSON).read)
    # Do more math to get to ~800 samples
    # Save the array of samples as self.waveformArray
    if skipAmount
      self.waveformArray = data_hash['data'].every_nth(skipAmount).collect { |n| n.to_f / 100 }
    else
      self.waveformArray = data_hash['data'].collect { |n| n.to_f / 100 }
    end
    self.save
    return true
  end


  def self.ingestUnparsedAudioRecording(origionalFilename, fullpath)
    puts "try to get the file that's passed"
    if !audioRecordingFile = File.open(fullpath)
      return false, "File could not be opened"
    end
#    audioRecordingFileMetadataTmp = File.open("/tmp/#{origionalFilename}.json")
    audioRecordingFileMetadata = {}
    audioRecordingFileMetadata['original'] = audioRecordingFile.getOSMetaDataObject
    audioRecordingFileMetadata['sox'] = `/usr/bin/sox --i "#{File.absolute_path(audioRecordingFile)}"`.split(/\n+/).each_with_object({}) { |s,h| key, value = s.scan(/([\w\s]+)\s+(.*)/).first; h[key.strip]=value.sub(/:/, ' ').strip if value}
    audioRecordingFileMetadata['durationInSeconds'] = `/usr/bin/sox --i -D "#{File.absolute_path(audioRecordingFile)}"`.strip.to_f
    
#    audioRecordingFileMetadataTmp = Tempfile.new([origionalFilename, ".json"])
    
#    audioRecordingFileMetadataTmp = File.new("#{origionalFilename}.json")
    File.open("/tmp/#{origionalFilename}.json", "w") {|f| f.write(JSON.pretty_generate(audioRecordingFileMetadata)) } #Automatically closes the file afterwards. The result is the number of bytes written, NOT the File object
#    audioRecordingFileMetadataTmp.close
#    metadataJSON.original_filename = "#{origionalFilename}.json"

#    puts metadataJSON
    puts "Calling sox"
    puts system "sox \"#{fullpath}\" -n spectrogram -r -o \"/tmp/#{origionalFilename}.png\""
    puts "Done calling sox?"
#    audioRecordingFileMetadataTmp.close
#    audioRecordingFileMetadataTmp << JSON.pretty_generate(audioRecordingFileMetadata)
#    audioRecordingFileMetadataTmp.rewind #Saves the file
   # puts audioRecordingFileMetadataTmp.read
    audioRecordingNode = AudioRecording.merge(name: origionalFilename)

    audioRecordingNode.ingestedFile = audioRecordingFile
    if audioRecordingFileMetadata['durationInSeconds']
      audioRecordingNode.durationInSeconds = audioRecordingFileMetadata['durationInSeconds']
    end
    audioRecordingNode.save
    audioRecordingFile.close

    if metadataFile = File.open("/tmp/#{origionalFilename}.json")
      audioRecordingNode.ingestedFileMetadata = metadataFile
#    audioRecordingNode.ingestedFileMetadata = audioRecordingFileMetadataTmp
      audioRecordingNode.save
      metadataFile.close
    else
      puts "Could not open '/tmp/#{origionalFilename}.json'"
    end

    visualizationFile = File.open("/tmp/#{origionalFilename}.png")
    audioRecordingNode.ingestedVisualizationFile = visualizationFile
    audioRecordingNode.save
    visualizationFile.close
    
#    audioRecordingNode.returnTrue2
#    audioRecordingNode.returnTrue
    audioRecordingNode.getWaveSurferArray

#    puts "audioRecordingNode.errors: #{audioRecordingNode.errors}"
#    puts "audioRecordingNode.errors.messages: #{audioRecordingNode.errors.messages}"
#    puts "audioRecordingNode.errors.inspect: #{audioRecordingNode.errors.inspect}"
#    puts audioRecordingNode.ingestedFileMetadata.errors.messages
#    puts audioRecordingNode.ingestedFileMetadata.errors.inspect
#    puts audioRecordingNode.save
#    audioRecordingFile.close
  end

  def self.ingestParsedAudioFolderOfRecordings()

    jsonIndex = File.join("**", "*.json")
    # puts jsonIndex ##jsonIndex was phased out. Now it's a json for each snippet and no index file

    Dir.glob(jsonIndex) {|file|
      # 'file' is the relative path and filename. As in 'soxflactemp.noconversion/index.json'
      puts file
      ingestParsedAudioRecording(file) #Sends index.json to the parsing function
      # Each index.json afterwards will be for different folders (hopefully)

      # puts jsonFile.absolute_path
      # if jsonFileParsed = JSON.parse(jsonFile)
      #     jsonFileParsed.each do |jsonFileParsedEntry|
      #         puts jsonFileParsedEntry['file']
      #         snippetJsonFile = File.read("#{jsonFileParsedEntry['file']}")
      #     end
      # end
    }

  end

end

class ITunesVoiceMemo < AudioRecording

end
